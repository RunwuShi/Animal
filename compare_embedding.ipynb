{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 95044608\n",
      "Trainable Parameters: 95044608\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from s3prl.nn import S3PRLUpstream\n",
    "\n",
    "torch.cuda.set_device(1)\n",
    "used_device =  torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# model = S3PRLUpstream(\"hubert\").to(used_device)\n",
    "model = S3PRLUpstream(\"wav2vec2\").to(used_device)\n",
    "# model = S3PRLUpstream(\"tera_100hr\").to(used_device)\n",
    "model.eval()\n",
    "\n",
    "def print_model_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total Parameters: {total_params}\")\n",
    "    print(f\"Trainable Parameters: {trainable_params}\")\n",
    "\n",
    "# print(model)\n",
    "print_model_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3PRLUpstream(\n",
      "  (upstream): UpstreamExpert(\n",
      "    (model): Wav2Vec2Model(\n",
      "      (feature_extractor): ConvFeatureExtractionModel(\n",
      "        (conv_layers): ModuleList(\n",
      "          (0): Sequential(\n",
      "            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
      "            (1): Dropout(p=0.0, inplace=False)\n",
      "            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)\n",
      "            (3): GELU(approximate='none')\n",
      "          )\n",
      "          (1-4): 4 x Sequential(\n",
      "            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
      "            (1): Dropout(p=0.0, inplace=False)\n",
      "            (2): GELU(approximate='none')\n",
      "          )\n",
      "          (5-6): 2 x Sequential(\n",
      "            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
      "            (1): Dropout(p=0.0, inplace=False)\n",
      "            (2): GELU(approximate='none')\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)\n",
      "      (dropout_input): Dropout(p=0.1, inplace=False)\n",
      "      (dropout_features): Dropout(p=0.1, inplace=False)\n",
      "      (quantizer): GumbelVectorQuantizer(\n",
      "        (weight_proj): Linear(in_features=512, out_features=640, bias=True)\n",
      "      )\n",
      "      (project_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (encoder): TransformerEncoder(\n",
      "        (pos_conv): Sequential(\n",
      "          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
      "          (1): SamePad()\n",
      "          (2): GELU(approximate='none')\n",
      "        )\n",
      "        (layers): ModuleList(\n",
      "          (0-11): 12 x TransformerSentenceEncoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (dropout_module): FairseqDropout()\n",
      "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (dropout1): Dropout(p=0.1, inplace=False)\n",
      "            (dropout2): Dropout(p=0.0, inplace=False)\n",
      "            (dropout3): Dropout(p=0.1, inplace=False)\n",
      "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (final_proj): Linear(in_features=768, out_features=256, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "print(model)\n",
    "num_layers = 0\n",
    "\n",
    "# # 遍历模型的所有模块和子模块\n",
    "# for layer in model.modules():\n",
    "#     # 检查当前模块是否是层的实例\n",
    "#     if isinstance(layer, nn.Module):\n",
    "#         num_layers += 1\n",
    "# # 打印层数\n",
    "# print(f\"Total number of layers in the model: {num_layers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 100, 768]) torch.Size([2])\n",
      "num_layers 13\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    wavs = torch.randn(2, 16000 * 2).to(used_device)\n",
    "    wavs_len = torch.LongTensor([16000 * 1, 16000 * 2]).to(used_device)\n",
    "    all_hs, all_hs_len = model(wavs, wavs_len)\n",
    "    num_layers = len(all_hs)\n",
    "\n",
    "for hs, hs_len in zip(all_hs, all_hs_len):\n",
    "    # assert isinstance(hs, torch.FloatTensor)\n",
    "    # assert isinstance(hs_len, torch.LongTensor)\n",
    "\n",
    "    batch_size, max_seq_len, hidden_size = hs.shape\n",
    "    assert hs_len.dim() == 1   \n",
    "    \n",
    "print(all_hs[0].shape,all_hs_len[0].shape)\n",
    "print('num_layers',num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 95044608\n",
      "Trainable Parameters: 95044608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20018/20018 [17:37<00:00, 18.94it/s]\n"
     ]
    }
   ],
   "source": [
    "######################################################wav2vec2######################################################\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from s3prl.nn import S3PRLUpstream\n",
    "\n",
    "torch.cuda.set_device(1)\n",
    "used_device =  torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# model = S3PRLUpstream(\"hubert\").to(used_device)\n",
    "model = S3PRLUpstream(\"wav2vec2\").to(used_device)\n",
    "model.eval()\n",
    "\n",
    "def print_model_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total Parameters: {total_params}\")\n",
    "    print(f\"Trainable Parameters: {trainable_params}\")\n",
    "\n",
    "# print(model)\n",
    "print_model_parameters(model)\n",
    "\n",
    "refer_info_path = '/mnt/work/dataset/preprocessed/great_bird/data_info'+'/'+'ori_test_info.json'\n",
    "# refer_info_path = '/mnt/work/dataset/preprocessed/great_bird/data_info'+'/'+'ori_data_info_caller.json'\n",
    "\n",
    "\n",
    "wav_data_path = '/mnt/work/dataset/preprocessed/great_bird/original_data'\n",
    "\n",
    "def process_subset_file(data_path):     \n",
    "    # total data list\n",
    "    data_path_list = [] # npy file path\n",
    "    c_id_index_list = []\n",
    "    cID_list = []\n",
    "    cID_type_list = []\n",
    "    with open(data_path, 'r') as f:\n",
    "        data_info = json.load(f)\n",
    "\n",
    "    for caller_ID in data_info:\n",
    "        for caller_type in data_info[caller_ID]:\n",
    "            for item in data_info[caller_ID][caller_type]:\n",
    "                \n",
    "                # reduce too len\n",
    "                # before 240214 \n",
    "                # if item['len_mel'] >= 199 or item['len_mel'] <= 99:\n",
    "                #     continue\n",
    "                \n",
    "                # after 240214\n",
    "                if item['len_mel'] >= 399 or item['len_mel'] <= 99:\n",
    "                    continue\n",
    "                \n",
    "                class_id_index = item['class_id_index']\n",
    "\n",
    "                # wav file path\n",
    "                wav_path = item['original_wav']\n",
    "                \n",
    "                data_path_list.append(wav_path)\n",
    "                # class_id_index eg 20221W67_0_0\n",
    "                # c_id_index_list.append(class_id_index)\n",
    "                # caller id\n",
    "                cID_list.append(caller_ID)\n",
    "                # caller type \n",
    "                cID_type_list.append(caller_type)\n",
    "                \n",
    "    return data_path_list, cID_list, cID_type_list\n",
    "\n",
    "\n",
    "data_path_list, cID_list, cID_type_list = process_subset_file(refer_info_path)\n",
    "\n",
    "\n",
    "def unpad(hs, hs_len, index, layer):\n",
    "    if layer == -1:\n",
    "        hs_unpadded = []\n",
    "        for h, lens in zip(hs, hs_len):\n",
    "            l = lens[index]\n",
    "            hs_unpadded.append(h[index, :l])\n",
    "    else:\n",
    "        l = hs_len[index]\n",
    "        hs_unpadded = hs[index, :l]\n",
    "    return hs_unpadded\n",
    "\n",
    "def get_batch(wav_paths, wav_ids, start, end):\n",
    "    def getitem(index):\n",
    "        import soundfile as sf\n",
    "\n",
    "        path = wav_paths[index]\n",
    "        wav, curr_sample_rate = sf.read(path, dtype=\"float32\")\n",
    "        wav /= np.max(np.abs(wav))\n",
    "\n",
    "        feats = torch.from_numpy(wav).float()\n",
    "        return feats\n",
    "\n",
    "    def collate(wavs, padding_value: int = 0):\n",
    "        from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "        padded_wavs = pad_sequence(\n",
    "            wavs, batch_first=True, padding_value=padding_value\n",
    "        )\n",
    "        return padded_wavs\n",
    "\n",
    "    end_id = min(end, len(wav_paths))\n",
    "    wavs = [getitem(index) for index in range(start, end_id)]\n",
    "    ids = [wav_ids[index] for index in range(start, end_id)]\n",
    "    wavs_len = [len(wav) for wav in wavs]\n",
    "    padded_wavs = collate(wavs)\n",
    "    \n",
    "    return padded_wavs, wavs_len, ids\n",
    "\n",
    "def get_features(model, wavs, wavs_len, layer=-1, device=\"cuda\"):\n",
    "    with torch.no_grad():\n",
    "        wavs_len = torch.LongTensor(wavs_len).to(device)\n",
    "        # print('wavs_len',wavs_len)\n",
    "        all_hs, all_hs_len = model(wavs.to(device), wavs_len)\n",
    "        # print('all_hs',all_hs[0].shape,'all_hs_len',all_hs_len)\n",
    "\n",
    "    for layer_id, (hs, hs_len) in enumerate(zip(all_hs, all_hs_len)):\n",
    "        hs = hs.to(\"cpu\")\n",
    "        hs_len = hs_len.to(\"cpu\")\n",
    "        assert isinstance(hs, torch.FloatTensor)\n",
    "        assert isinstance(hs_len, torch.LongTensor)\n",
    "\n",
    "        if layer == layer:\n",
    "            hidden_states = hs\n",
    "            hidden_states_len = hs_len\n",
    "\n",
    "        assert hs_len.dim() == 1\n",
    "\n",
    "    if layer == -1:\n",
    "        hidden_states = [hs.to(\"cpu\") for hs in all_hs]\n",
    "        hidden_states_len = [hs_len.to(\"cpu\") for hs_len in all_hs_len]\n",
    "        \n",
    "    return hidden_states, hidden_states_len\n",
    "\n",
    "def pack_features(hidden_states, hidden_states_len, ids, layer, features_dict, features_list):\n",
    "    num_inputs = len(ids)\n",
    "    for input_id in range(num_inputs):\n",
    "        wav_id = ids[input_id]\n",
    "        \n",
    "        # print(hidden_states_len, input_id, layer)\n",
    "        hs = unpad(hidden_states, hidden_states_len, input_id, layer)\n",
    "        features_dict[wav_id] = hs\n",
    "        \n",
    "        features_list.append(hs)\n",
    "        # features_list_1D.append(hs)\n",
    "        \n",
    "    return features_dict, features_list\n",
    "\n",
    "def extract_features(model, wav_paths, wav_ids, batch_size, layer_id, device=\"cuda\"):\n",
    "    num_wavs = len(wav_paths)\n",
    "    features_dict = {}\n",
    "    features_list = []\n",
    "    \n",
    "    means_total = []\n",
    "    std_total = []\n",
    "    mean_std_total = []\n",
    "    uid_total = []\n",
    "    \n",
    "    i = 0\n",
    "    for bid in tqdm(range(0, num_wavs, batch_size)):\n",
    "        # if i >= 5:\n",
    "        #     break\n",
    "        i += 1\n",
    "        end_id = min(bid + batch_size, num_wavs)\n",
    "        \n",
    "        padded_wavs, wavs_len, ids = get_batch(wav_paths, wav_ids, bid, end_id)\n",
    "        hidden_states, hidden_states_len = get_features(model, padded_wavs, wavs_len, layer_id, used_device)\n",
    "        \n",
    "        # print('hidden_states',hidden_states.shape)\n",
    "        # print(ids)\n",
    "        \n",
    "        hidden_states = hidden_states.squeeze(dim=0)\n",
    "        \n",
    "        means = torch.mean(hidden_states, dim=0)\n",
    "        means_total.append(means.numpy())\n",
    "        \n",
    "        stds = torch.std(hidden_states, dim=0)\n",
    "        std_total.append(stds.numpy())\n",
    "        \n",
    "        mean_std = torch.cat((means, stds), dim=0)\n",
    "        mean_std_total.append(mean_std.numpy())\n",
    "        \n",
    "        uid_total.append(ids)\n",
    "        \n",
    "        features_dict, features_list = pack_features(hidden_states, hidden_states_len, ids, layer_id, features_dict, features_list)\n",
    "        \n",
    "    return features_dict, features_list, means_total, std_total, mean_std_total,  uid_total\n",
    "\n",
    "# output [batch size, time length, 768]\n",
    "features_dict, features_list, means_total, std_total, mean_std_total, uid_total = extract_features(\n",
    "            model = model,\n",
    "            wav_paths = data_path_list,\n",
    "            wav_ids = cID_type_list,\n",
    "            batch_size = 1,\n",
    "            layer_id = 12, # -1 all layers\n",
    "            device = used_device\n",
    "        )\n",
    "\n",
    "# means_total = []\n",
    "# std_total = []\n",
    "# mean_std_total = []\n",
    "# uid_total = []\n",
    "\n",
    "# for uid, rep in features_dict.items():\n",
    "#     print(rep.shape)\n",
    "    \n",
    "#     # means and stds\n",
    "#     means = torch.mean(rep, dim=0)\n",
    "#     means_total.append(means.numpy())\n",
    "    \n",
    "#     stds = torch.std(rep, dim=0)\n",
    "#     std_total.append(stds.numpy())\n",
    "    \n",
    "#     mean_std = torch.cat((means, stds), dim=0)\n",
    "#     mean_std_total.append(mean_std)\n",
    "    \n",
    "#     uid_total.append(uid)\n",
    "    \n",
    "    # print('uid', uid)\n",
    "    # print('means',means_total[0])\n",
    "    # print('stds',std_total[0])\n",
    "    \n",
    "# print(len(means_total[1]))\n",
    "# save the embeddings\n",
    "# save path\n",
    "\n",
    "#################################################wav2vec2#################################################\n",
    "save_path = '/mnt/work/Animal/output/embeddings/wav2vec2/first_ex'\n",
    "\n",
    "em_save = True\n",
    "# em_save = False\n",
    "if em_save:\n",
    "    save_df = pd.DataFrame(means_total)\n",
    "    save_df.to_csv(os.path.join(save_path, 'embedding.csv'), index=False, header=False)\n",
    "    print(save_df.head())\n",
    "    \n",
    "    caller_type_df = pd.DataFrame(uid_total) \n",
    "    caller_type_df.to_csv(os.path.join(save_path, 'caller_type_label.csv'), index=False, header=False)\n",
    "    print(caller_type_df.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 94697600\n",
      "Trainable Parameters: 94697600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20018/20018 [13:26<00:00, 24.82it/s]\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: '/mnt/work/Animal/output/embeddings/hubert/first_ex'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 232\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m em_save:\n\u001b[1;32m    231\u001b[0m     save_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(means_total)\n\u001b[0;32m--> 232\u001b[0m     \u001b[43msave_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43membedding.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28mprint\u001b[39m(save_df\u001b[38;5;241m.\u001b[39mhead())\n\u001b[1;32m    235\u001b[0m     caller_type_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(uid_total) \n",
      "File \u001b[0;32m~/.conda/envs/work/lib/python3.10/site-packages/pandas/core/generic.py:3902\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3891\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[1;32m   3893\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[1;32m   3894\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[1;32m   3895\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3899\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[1;32m   3900\u001b[0m )\n\u001b[0;32m-> 3902\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3903\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3904\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3905\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3906\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3907\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3908\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3909\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3910\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3911\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3912\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3913\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3914\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/work/lib/python3.10/site-packages/pandas/io/formats/format.py:1152\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1131\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[1;32m   1134\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[1;32m   1135\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[1;32m   1151\u001b[0m )\n\u001b[0;32m-> 1152\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[1;32m   1155\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[0;32m~/.conda/envs/work/lib/python3.10/site-packages/pandas/io/formats/csvs.py:247\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[0;32m--> 247\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[1;32m    257\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[1;32m    258\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[1;32m    264\u001b[0m     )\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[0;32m~/.conda/envs/work/lib/python3.10/site-packages/pandas/io/common.py:739\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    737\u001b[0m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[0;32m--> 739\u001b[0m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzstd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    743\u001b[0m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/work/lib/python3.10/site-packages/pandas/io/common.py:604\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    602\u001b[0m parent \u001b[38;5;241m=\u001b[39m Path(path)\u001b[38;5;241m.\u001b[39mparent\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[0;32m--> 604\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot save file into a non-existent directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mOSError\u001b[0m: Cannot save file into a non-existent directory: '/mnt/work/Animal/output/embeddings/hubert/first_ex'"
     ]
    }
   ],
   "source": [
    "######################################################hubert######################################################\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from s3prl.nn import S3PRLUpstream\n",
    "\n",
    "torch.cuda.set_device(1)\n",
    "used_device =  torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = S3PRLUpstream(\"hubert\").to(used_device)\n",
    "model.eval()\n",
    "\n",
    "def print_model_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total Parameters: {total_params}\")\n",
    "    print(f\"Trainable Parameters: {trainable_params}\")\n",
    "\n",
    "# print(model)\n",
    "print_model_parameters(model)\n",
    "\n",
    "refer_info_path = '/mnt/work/dataset/preprocessed/great_bird/data_info'+'/'+'ori_test_info.json'\n",
    "# refer_info_path = '/mnt/work/dataset/preprocessed/great_bird/data_info'+'/'+'ori_data_info_caller.json'\n",
    "\n",
    "wav_data_path = '/mnt/work/dataset/preprocessed/great_bird/original_data'\n",
    "\n",
    "def process_subset_file(data_path):     \n",
    "    # total data list\n",
    "    data_path_list = [] # npy file path\n",
    "    c_id_index_list = []\n",
    "    cID_list = []\n",
    "    cID_type_list = []\n",
    "    with open(data_path, 'r') as f:\n",
    "        data_info = json.load(f)\n",
    "\n",
    "    for caller_ID in data_info:\n",
    "        for caller_type in data_info[caller_ID]:\n",
    "            for item in data_info[caller_ID][caller_type]:\n",
    "                \n",
    "                # reduce too len\n",
    "                # before 240214 \n",
    "                # if item['len_mel'] >= 199 or item['len_mel'] <= 99:\n",
    "                #     continue\n",
    "                \n",
    "                # after 240214\n",
    "                if item['len_mel'] >= 399 or item['len_mel'] <= 99:\n",
    "                    continue\n",
    "                \n",
    "                class_id_index = item['class_id_index']\n",
    "\n",
    "                # wav file path\n",
    "                wav_path = item['original_wav']\n",
    "                \n",
    "                data_path_list.append(wav_path)\n",
    "                # class_id_index eg 20221W67_0_0\n",
    "                # c_id_index_list.append(class_id_index)\n",
    "                # caller id\n",
    "                cID_list.append(caller_ID)\n",
    "                # caller type \n",
    "                cID_type_list.append(caller_type)\n",
    "                \n",
    "    return data_path_list, cID_list, cID_type_list\n",
    "\n",
    "\n",
    "data_path_list, cID_list, cID_type_list = process_subset_file(refer_info_path)\n",
    "\n",
    "\n",
    "def unpad(hs, hs_len, index, layer):\n",
    "    if layer == -1:\n",
    "        hs_unpadded = []\n",
    "        for h, lens in zip(hs, hs_len):\n",
    "            l = lens[index]\n",
    "            hs_unpadded.append(h[index, :l])\n",
    "    else:\n",
    "        l = hs_len[index]\n",
    "        hs_unpadded = hs[index, :l]\n",
    "    return hs_unpadded\n",
    "\n",
    "def get_batch(wav_paths, wav_ids, start, end):\n",
    "    def getitem(index):\n",
    "        import soundfile as sf\n",
    "\n",
    "        path = wav_paths[index]\n",
    "        wav, curr_sample_rate = sf.read(path, dtype=\"float32\")\n",
    "        wav /= np.max(np.abs(wav))\n",
    "\n",
    "        feats = torch.from_numpy(wav).float()\n",
    "        return feats\n",
    "\n",
    "    def collate(wavs, padding_value: int = 0):\n",
    "        from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "        padded_wavs = pad_sequence(\n",
    "            wavs, batch_first=True, padding_value=padding_value\n",
    "        )\n",
    "        return padded_wavs\n",
    "\n",
    "    end_id = min(end, len(wav_paths))\n",
    "    wavs = [getitem(index) for index in range(start, end_id)]\n",
    "    ids = [wav_ids[index] for index in range(start, end_id)]\n",
    "    wavs_len = [len(wav) for wav in wavs]\n",
    "    padded_wavs = collate(wavs)\n",
    "    \n",
    "    return padded_wavs, wavs_len, ids\n",
    "\n",
    "def get_features(model, wavs, wavs_len, layer=-1, device=\"cuda\"):\n",
    "    with torch.no_grad():\n",
    "        wavs_len = torch.LongTensor(wavs_len).to(device)\n",
    "        # print('wavs_len',wavs_len)\n",
    "        all_hs, all_hs_len = model(wavs.to(device), wavs_len)\n",
    "        # print('all_hs',all_hs[0].shape,'all_hs_len',all_hs_len)\n",
    "\n",
    "    for layer_id, (hs, hs_len) in enumerate(zip(all_hs, all_hs_len)):\n",
    "        hs = hs.to(\"cpu\")\n",
    "        hs_len = hs_len.to(\"cpu\")\n",
    "        assert isinstance(hs, torch.FloatTensor)\n",
    "        assert isinstance(hs_len, torch.LongTensor)\n",
    "\n",
    "        if layer == layer:\n",
    "            hidden_states = hs\n",
    "            hidden_states_len = hs_len\n",
    "\n",
    "        assert hs_len.dim() == 1\n",
    "\n",
    "    if layer == -1:\n",
    "        hidden_states = [hs.to(\"cpu\") for hs in all_hs]\n",
    "        hidden_states_len = [hs_len.to(\"cpu\") for hs_len in all_hs_len]\n",
    "        \n",
    "    return hidden_states, hidden_states_len\n",
    "\n",
    "def pack_features(hidden_states, hidden_states_len, ids, layer, features_dict, features_list):\n",
    "    num_inputs = len(ids)\n",
    "    for input_id in range(num_inputs):\n",
    "        wav_id = ids[input_id]\n",
    "        \n",
    "        # print(hidden_states_len, input_id, layer)\n",
    "        hs = unpad(hidden_states, hidden_states_len, input_id, layer)\n",
    "        features_dict[wav_id] = hs\n",
    "        \n",
    "        features_list.append(hs)\n",
    "        # features_list_1D.append(hs)\n",
    "        \n",
    "    return features_dict, features_list\n",
    "\n",
    "def extract_features(model, wav_paths, wav_ids, batch_size, layer_id, device=\"cuda\"):\n",
    "    num_wavs = len(wav_paths)\n",
    "    features_dict = {}\n",
    "    features_list = []\n",
    "    \n",
    "    means_total = []\n",
    "    std_total = []\n",
    "    mean_std_total = []\n",
    "    uid_total = []\n",
    "    \n",
    "    i = 0\n",
    "    for bid in tqdm(range(0, num_wavs, batch_size)):\n",
    "        # if i >= 5:\n",
    "        #     break\n",
    "        i += 1\n",
    "        end_id = min(bid + batch_size, num_wavs)\n",
    "        \n",
    "        padded_wavs, wavs_len, ids = get_batch(wav_paths, wav_ids, bid, end_id)\n",
    "        hidden_states, hidden_states_len = get_features(model, padded_wavs, wavs_len, layer_id, used_device)\n",
    "        \n",
    "        # print('hidden_states',hidden_states.shape)\n",
    "        # print(ids)\n",
    "        \n",
    "        hidden_states = hidden_states.squeeze(dim=0)\n",
    "        \n",
    "        means = torch.mean(hidden_states, dim=0)\n",
    "        means_total.append(means.numpy())\n",
    "        \n",
    "        stds = torch.std(hidden_states, dim=0)\n",
    "        std_total.append(stds.numpy())\n",
    "        \n",
    "        mean_std = torch.cat((means, stds), dim=0)\n",
    "        mean_std_total.append(mean_std.numpy())\n",
    "        \n",
    "        uid_total.append(ids)\n",
    "        \n",
    "        features_dict, features_list = pack_features(hidden_states, hidden_states_len, ids, layer_id, features_dict, features_list)\n",
    "        \n",
    "    return features_dict, features_list, means_total, std_total, mean_std_total,  uid_total\n",
    "\n",
    "# output [batch size, time length, 768]\n",
    "features_dict, features_list, means_total, std_total, mean_std_total, uid_total = extract_features(\n",
    "            model = model,\n",
    "            wav_paths = data_path_list,\n",
    "            wav_ids = cID_type_list,\n",
    "            batch_size = 1,\n",
    "            layer_id = 12, # -1 all layers\n",
    "            device = used_device\n",
    "        )\n",
    "\n",
    "# means_total = []\n",
    "# std_total = []\n",
    "# mean_std_total = []\n",
    "# uid_total = []\n",
    "\n",
    "# for uid, rep in features_dict.items():\n",
    "#     print(rep.shape)\n",
    "    \n",
    "#     # means and stds\n",
    "#     means = torch.mean(rep, dim=0)\n",
    "#     means_total.append(means.numpy())\n",
    "    \n",
    "#     stds = torch.std(rep, dim=0)\n",
    "#     std_total.append(stds.numpy())\n",
    "    \n",
    "#     mean_std = torch.cat((means, stds), dim=0)\n",
    "#     mean_std_total.append(mean_std)\n",
    "    \n",
    "#     uid_total.append(uid)\n",
    "    \n",
    "    # print('uid', uid)\n",
    "    # print('means',means_total[0])\n",
    "    # print('stds',std_total[0])\n",
    "    \n",
    "# print(len(means_total[1]))\n",
    "# save the embeddings\n",
    "# save path\n",
    "save_path = '/mnt/work/Animal/output/embeddings/hubert/first_ex'\n",
    "\n",
    "em_save = True\n",
    "# em_save = False\n",
    "if em_save:\n",
    "    save_df = pd.DataFrame(means_total)\n",
    "    save_df.to_csv(os.path.join(save_path, 'embedding.csv'), index=False, header=False)\n",
    "    print(save_df.head())\n",
    "    \n",
    "    caller_type_df = pd.DataFrame(uid_total) \n",
    "    caller_type_df.to_csv(os.path.join(save_path, 'caller_type_label.csv'), index=False, header=False)\n",
    "    print(caller_type_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 4630096\n",
      "Trainable Parameters: 4630096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20018/20018 [11:40<00:00, 28.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0         1         2         3         4         5         6    \\\n",
      "0  0.671216  0.589576 -0.779488 -0.407928 -0.257567 -0.544122 -0.991560   \n",
      "1  0.735563  0.751959 -0.653569 -0.427348 -0.463505 -0.302425 -1.059371   \n",
      "2  0.711596  0.530423 -0.895867 -0.525112 -0.434542 -0.171986 -1.082592   \n",
      "3  0.837570  0.883121 -0.830981 -0.392587 -0.226456 -0.279167 -1.066869   \n",
      "4  0.563029  0.687522 -0.653995 -0.474979 -0.207672 -0.269855 -0.934507   \n",
      "\n",
      "        7         8         9    ...       502       503       504       505  \\\n",
      "0  0.616165 -0.582342  0.992087  ... -1.142268 -0.842943  0.814530  0.189002   \n",
      "1  0.360494 -0.849294  0.961290  ... -1.110213 -0.544218  0.813327  0.299149   \n",
      "2  0.442133 -0.756097  0.593512  ... -1.144335 -0.644250  0.809140  0.529178   \n",
      "3  0.640621 -0.738344  0.961360  ... -1.176123 -0.720220  0.897317  0.117179   \n",
      "4  0.580482 -0.832433  1.138621  ... -1.246126 -0.677829  0.969136  0.369084   \n",
      "\n",
      "        506       507       508       509       510       511  \n",
      "0  0.489539  1.088973  0.304456 -1.209350  0.558848  0.338149  \n",
      "1  0.450876  0.999404  0.189333 -1.063576  0.613622  0.308981  \n",
      "2  0.680215  1.001994  0.163490 -1.085028  0.851330  0.307859  \n",
      "3  0.484395  1.129490  0.275822 -1.129032  0.604166  0.541503  \n",
      "4  0.582860  0.987105  0.125221 -1.270194  0.816273  0.558530  \n",
      "\n",
      "[5 rows x 512 columns]\n",
      "       0\n",
      "0  W67_0\n",
      "1  W67_0\n",
      "2  W67_0\n",
      "3  W67_0\n",
      "4  W67_0\n"
     ]
    }
   ],
   "source": [
    "######################################################vq_apc######################################################\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from s3prl.nn import S3PRLUpstream\n",
    "\n",
    "# torch.cuda.set_device(1)\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "used_device =  torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = S3PRLUpstream(\"vq_apc\").to(used_device)\n",
    "model.eval()\n",
    "\n",
    "def print_model_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total Parameters: {total_params}\")\n",
    "    print(f\"Trainable Parameters: {trainable_params}\")\n",
    "\n",
    "# print(model)\n",
    "print_model_parameters(model)\n",
    "\n",
    "refer_info_path = '/mnt/work/dataset/preprocessed/great_bird/data_info'+'/'+'ori_test_info.json'\n",
    "# refer_info_path = '/mnt/work/dataset/preprocessed/great_bird/data_info'+'/'+'ori_data_info_caller.json'\n",
    "wav_data_path = '/mnt/work/dataset/preprocessed/great_bird/original_data'\n",
    "\n",
    "def process_subset_file(data_path):     \n",
    "    # total data list\n",
    "    data_path_list = [] # npy file path\n",
    "    c_id_index_list = []\n",
    "    cID_list = []\n",
    "    cID_type_list = []\n",
    "    with open(data_path, 'r') as f:\n",
    "        data_info = json.load(f)\n",
    "\n",
    "    for caller_ID in data_info:\n",
    "        for caller_type in data_info[caller_ID]:\n",
    "            for item in data_info[caller_ID][caller_type]:\n",
    "                \n",
    "                # reduce too len\n",
    "                # before 240214 \n",
    "                # if item['len_mel'] >= 199 or item['len_mel'] <= 99:\n",
    "                #     continue\n",
    "                \n",
    "                # after 240214\n",
    "                if item['len_mel'] >= 399 or item['len_mel'] <= 99:\n",
    "                    continue\n",
    "                \n",
    "                class_id_index = item['class_id_index']\n",
    "\n",
    "                # wav file path\n",
    "                wav_path = item['original_wav']\n",
    "                \n",
    "                data_path_list.append(wav_path)\n",
    "                # class_id_index eg 20221W67_0_0\n",
    "                # c_id_index_list.append(class_id_index)\n",
    "                # caller id\n",
    "                cID_list.append(caller_ID)\n",
    "                # caller type \n",
    "                cID_type_list.append(caller_type)\n",
    "                \n",
    "    return data_path_list, cID_list, cID_type_list\n",
    "\n",
    "\n",
    "data_path_list, cID_list, cID_type_list = process_subset_file(refer_info_path)\n",
    "\n",
    "\n",
    "def unpad(hs, hs_len, index, layer):\n",
    "    if layer == -1:\n",
    "        hs_unpadded = []\n",
    "        for h, lens in zip(hs, hs_len):\n",
    "            l = lens[index]\n",
    "            hs_unpadded.append(h[index, :l])\n",
    "    else:\n",
    "        l = hs_len[index]\n",
    "        hs_unpadded = hs[index, :l]\n",
    "    return hs_unpadded\n",
    "\n",
    "def get_batch(wav_paths, wav_ids, start, end):\n",
    "    def getitem(index):\n",
    "        import soundfile as sf\n",
    "\n",
    "        path = wav_paths[index]\n",
    "        wav, curr_sample_rate = sf.read(path, dtype=\"float32\")\n",
    "        wav /= np.max(np.abs(wav))\n",
    "\n",
    "        feats = torch.from_numpy(wav).float()\n",
    "        return feats\n",
    "\n",
    "    def collate(wavs, padding_value: int = 0):\n",
    "        from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "        padded_wavs = pad_sequence(\n",
    "            wavs, batch_first=True, padding_value=padding_value\n",
    "        )\n",
    "        return padded_wavs\n",
    "\n",
    "    end_id = min(end, len(wav_paths))\n",
    "    wavs = [getitem(index) for index in range(start, end_id)]\n",
    "    ids = [wav_ids[index] for index in range(start, end_id)]\n",
    "    wavs_len = [len(wav) for wav in wavs]\n",
    "    padded_wavs = collate(wavs)\n",
    "    \n",
    "    return padded_wavs, wavs_len, ids\n",
    "\n",
    "def get_features(model, wavs, wavs_len, layer=-1, device=\"cuda\"):\n",
    "    with torch.no_grad():\n",
    "        wavs_len = torch.LongTensor(wavs_len).to(device)\n",
    "        # print('wavs_len',wavs_len)\n",
    "        all_hs, all_hs_len = model(wavs.to(device), wavs_len)\n",
    "        # print('all_hs',all_hs[0].shape,'all_hs_len',all_hs_len)\n",
    "\n",
    "    for layer_id, (hs, hs_len) in enumerate(zip(all_hs, all_hs_len)):\n",
    "        hs = hs.to(\"cpu\")\n",
    "        hs_len = hs_len.to(\"cpu\")\n",
    "        assert isinstance(hs, torch.FloatTensor)\n",
    "        assert isinstance(hs_len, torch.LongTensor)\n",
    "\n",
    "        if layer == layer:\n",
    "            hidden_states = hs\n",
    "            hidden_states_len = hs_len\n",
    "\n",
    "        assert hs_len.dim() == 1\n",
    "\n",
    "    if layer == -1:\n",
    "        hidden_states = [hs.to(\"cpu\") for hs in all_hs]\n",
    "        hidden_states_len = [hs_len.to(\"cpu\") for hs_len in all_hs_len]\n",
    "        \n",
    "    return hidden_states, hidden_states_len\n",
    "\n",
    "def pack_features(hidden_states, hidden_states_len, ids, layer, features_dict, features_list):\n",
    "    num_inputs = len(ids)\n",
    "    for input_id in range(num_inputs):\n",
    "        wav_id = ids[input_id]\n",
    "        \n",
    "        # print(hidden_states_len, input_id, layer)\n",
    "        hs = unpad(hidden_states, hidden_states_len, input_id, layer)\n",
    "        features_dict[wav_id] = hs\n",
    "        \n",
    "        features_list.append(hs)\n",
    "        # features_list_1D.append(hs)\n",
    "        \n",
    "    return features_dict, features_list\n",
    "\n",
    "def extract_features(model, wav_paths, wav_ids, batch_size, layer_id, device=\"cuda\"):\n",
    "    num_wavs = len(wav_paths)\n",
    "    features_dict = {}\n",
    "    features_list = []\n",
    "    \n",
    "    means_total = []\n",
    "    std_total = []\n",
    "    mean_std_total = []\n",
    "    uid_total = []\n",
    "    \n",
    "    i = 0\n",
    "    for bid in tqdm(range(0, num_wavs, batch_size)):\n",
    "        # if i >= 5:\n",
    "        #     break\n",
    "        i += 1\n",
    "        end_id = min(bid + batch_size, num_wavs)\n",
    "        \n",
    "        padded_wavs, wavs_len, ids = get_batch(wav_paths, wav_ids, bid, end_id)\n",
    "        hidden_states, hidden_states_len = get_features(model, padded_wavs, wavs_len, layer_id, used_device)\n",
    "        \n",
    "        # print('hidden_states',hidden_states.shape)\n",
    "        # print(ids)\n",
    "        \n",
    "        hidden_states = hidden_states.squeeze(dim=0)\n",
    "        \n",
    "        means = torch.mean(hidden_states, dim=0)\n",
    "        means_total.append(means.numpy())\n",
    "        \n",
    "        stds = torch.std(hidden_states, dim=0)\n",
    "        std_total.append(stds.numpy())\n",
    "        \n",
    "        mean_std = torch.cat((means, stds), dim=0)\n",
    "        mean_std_total.append(mean_std.numpy())\n",
    "        \n",
    "        uid_total.append(ids)\n",
    "        \n",
    "        features_dict, features_list = pack_features(hidden_states, hidden_states_len, ids, layer_id, features_dict, features_list)\n",
    "        \n",
    "    return features_dict, features_list, means_total, std_total, mean_std_total,  uid_total\n",
    "\n",
    "# output [batch size, time length, 768]\n",
    "features_dict, features_list, means_total, std_total, mean_std_total, uid_total = extract_features(\n",
    "            model = model,\n",
    "            wav_paths = data_path_list,\n",
    "            wav_ids = cID_type_list,\n",
    "            batch_size = 1,\n",
    "            layer_id = 12, # -1 all layers\n",
    "            device = used_device\n",
    "        )\n",
    "\n",
    "# means_total = []\n",
    "# std_total = []\n",
    "# mean_std_total = []\n",
    "# uid_total = []\n",
    "\n",
    "# for uid, rep in features_dict.items():\n",
    "#     print(rep.shape)\n",
    "    \n",
    "#     # means and stds\n",
    "#     means = torch.mean(rep, dim=0)\n",
    "#     means_total.append(means.numpy())\n",
    "    \n",
    "#     stds = torch.std(rep, dim=0)\n",
    "#     std_total.append(stds.numpy())\n",
    "    \n",
    "#     mean_std = torch.cat((means, stds), dim=0)\n",
    "#     mean_std_total.append(mean_std)\n",
    "    \n",
    "#     uid_total.append(uid)\n",
    "    \n",
    "    # print('uid', uid)\n",
    "    # print('means',means_total[0])\n",
    "    # print('stds',std_total[0])\n",
    "    \n",
    "# print(len(means_total[1]))\n",
    "# save the embeddings\n",
    "# save path\n",
    "save_path = '/mnt/work/Animal/output/embeddings/vq_apc/first_ex'\n",
    "\n",
    "em_save = True\n",
    "# em_save = False\n",
    "if em_save:\n",
    "    save_df = pd.DataFrame(means_total)\n",
    "    save_df.to_csv(os.path.join(save_path, 'embedding.csv'), index=False, header=False)\n",
    "    print(save_df.head())\n",
    "    \n",
    "    caller_type_df = pd.DataFrame(uid_total) \n",
    "    caller_type_df.to_csv(os.path.join(save_path, 'caller_type_label.csv'), index=False, header=False)\n",
    "    print(caller_type_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the first row: 512\n"
     ]
    }
   ],
   "source": [
    "length_of_first_row = save_df.shape[1]\n",
    "print(\"Length of the first row:\", length_of_first_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B101_0 270\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "features_dict.keys()\n",
    "\n",
    "for uid, rep in features_dict.items(): \n",
    "    print(uid,len(rep))\n",
    "    for l in range(len(rep)):\n",
    "        mean = torch.mean(rep[l], dim=0)\n",
    "        print(mean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(features_list)\n",
    "# features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "import os\n",
    "\n",
    "print(data_path_list[0])\n",
    "print(len(data_path_list))\n",
    "print(cID_type_list[0])\n",
    "\n",
    "file_path = '/mnt/work/dataset/preprocessed/great_bird/original_data/20221B101_0_69.wav'\n",
    "if os.path.exists(file_path):\n",
    "    print(f\"The file {file_path} exists.\")\n",
    "else:\n",
    "    print(f\"The file {file_path} does not exist.\")\n",
    "\n",
    "wav, curr_sample_rate = sf.read('/mnt/work/dataset/preprocessed/great_bird/original_data/20221B101_0_69.wav', dtype=\"float32\")\n",
    "print(len(wav))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
